### 목표

1. 레이어 개념 이해
2. 딥러닝 모델 속 각 레이어 (Embedding, RNN, LSTM)의 동작 방식 이해
3. 데이터의 특성을 고려한 레이어 설계, 이를 Tensorflow로 정의하는 법 배움

### Sparse Representation (희소 표현)

벡터의 특정 차원에 단어 혹은 의미를 직접 매핑하는 방식

ex) 모양 (0둥글, 1길쭉), 색상 (0빨강, 1노랑)

사과 [0, 0], 바나나 [1,1], 배 [0,1]

### Distribution Hypothesis (분포 가설)

유사한 맥락에서 나타나는 단어는 의미도 비슷하다

### Distributed Representation (분산 표현)

유사한 맥락에 나타나는 단어들끼리는 두 단어 벡터 사이의 거리를 가깝게 하고, 그렇지 않은 단어들끼리는 멀어지도록 조금씩 조정해 주는 것

- 비단 단어를 표현하는 데에만 사용하는 것이 아니라, 의미적 유사성을 가지는 여러가지 것들을 컴퓨터에게 가르침

### Difference bettwen SR and DR

DR은 SR과 다르게 단어 간의 유사도를 **계산**으로 구할 수 있음

### Embedding Layer

- 단어의 분산 표현을 구하기 위한 레이어
- Weight이자 파라미터
- 다른 레이어가 그랬듯이 수많은 데이터를 통해 적합한 파라미터를 찾아감

### Embedding 레이어만을 훈련하기 위한 방법

- 신경망 훈련 도중 업데이트되는 것이 아니라 Embedding 레이어를 훈련하기 위한 방법
    
    ELMo(2018)
    
    - 언어 모델로 하는 임베딩
    - 사전 훈련된 언어 모델
    
    Word2Vec(2013)
    
    Glove(2014)
    
    - Word2Vec의 단점을 언급하며 더욱 나은 방법론 소개
    
    FastText(2016)
    

![스크린샷 2021-10-29 오전 10.33.13.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/2da8b233-30b7-4d9b-b5ee-c93f2fa763fb/스크린샷_2021-10-29_오전_10.33.13.png)

### 원-핫 인코딩

- 하나만 라벨링 1을 하고 나머지는 0으로 채움
- 차원 수가 일정 수준을 넘어가면 분류기의 성능은 되려 0으로 수렴하고, 아무리 뛰어난 성능을 가진 컴퓨터도 이런 고차원 벡터를 학습하기는 어렵고 성능이 떨어짐
- 원-핫 벡터로 표현된 서로 다른 두 단어는 항상 직교한다
    
    → 모든 단어가 단어 간 관계를 전혀 반영하지 못한 채 각각 독립적인 상태로 존재한다.
    

### Embeddinglayer

- 기본적으로 딥러닝은 미분을 기반으로 동작하는데, Embedding 레이어는 그저 단어를 대응 시켜 줄 뿐이니 미분이 불가능
- 신경망을 설계를 할 때, 어떤 연산 결과를 Embedding 레이어에 연결시키는 것은 불가능
    - 정확히 이해하고자하면 어려움
- **입력에 직접 연결되게 사용해야한다**
    
    → 그 입력은 원-핫 인코딩된 단어 벡터의 형태일 때가 이상적이다.
    

### RNN

- Sequential
- 순차적인 특성을 필수로 갖는다:Vanishing Gradinet 문제가 발생함

### 포럼

**21-2**

**그럼 이런 방법은 어떨까요? 단 하나의 정수로 단어를 표현하는 대신, 2차원 이상의 벡터로 단어를 표현하는 거죠! 위의 과일 친구들은 사과: `[ 0, 0 ]` , 바나나: `[ 1, 1 ]` , 배: `[ 0, 1 ]` 정도로 표현할 수 있겠네요. 첫 번째 요소는 모양(0:둥글다, 1:길쭉하다)을 나타내고, 두 번째 모양은 색상(0:빨강, 1:노랑)을 나타내는 거죠! 배는 모양 기준으로는 사과와 가깝고, 색상 기준으로는 바나나와 가깝다는 것이 아주 잘 표현되고 있습니다. 😎**

→ 두번째 모양이 아니라 두번째 요소