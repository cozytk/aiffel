# 알게된 것

## 워드 임베딩

- 선형변환을 통해 **단어의 특성**을 저차원상의 실수 벡터값으로 나타내는 방법 ****

## 노드 속 질문

- 텍스트를 어떻게 숫자 행렬로 표현할 수 있나요?
    - ascii형태로 이차행렬로 표현한 다음에 단어를 tokenzie한다
- 텍스트에는 순서가 중요합니다. 입력 데이터의 순서를 인공지능 모델에 어떻게 반영해야 하나요?
    - 입력데이터의 출력을 다음 입력데이터에 반영한다.

## 단어의 의미를 나타내는 벡터를 짝짓기

- 단어의 의미를 나타내는 벡터를 훈련 가능한 파라미터로 놓고 이를 딥러닝을 통해 학습해서 최적화
    
    ![스크린샷 2021-10-14 오전 11.08.25.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/bfb2b5d9-7d44-4fcc-9d14-5c3bdd33787e/스크린샷_2021-10-14_오전_11.08.25.png)
    

## `tf.keras.layers.Embedding(input_dim=, output_dim=)`

- input_dim 은 입력 데이터의 개수
- output_dim은 토큰화된 한 단어를 표현할 벡터의 길이

### RNN에서 다루는 데이터 형태

- Recurrent Neural Network(RNN)
- RNN은 시퀀스(Sequence) 형태의 데이터를 처리하기에 최적인 모델로 알려져 있음

### State machine

- RNN은 시간의 흐름에 따라 새롭게 들어오는 입력에 따라 변하는 현재 상태를 묘사
    
    ![스크린샷 2021-10-14 오전 11.49.15.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/26474a6b-6cb2-4754-a043-96e36584e187/스크린샷_2021-10-14_오전_11.49.15.png)
    

[rnn](https://www.notion.so/rnn-20c08ab82be34c9f98adadeb68a95d92)

- NN의 꽃 RNN (모두를 위한 딥러닝.sung kim)
    
    

### CNN 텍스트분류.

- 1-D CNN으로 텍스트분류를 해도 좋은 성능이 나온다.
    - 1차원 2차원을 구분하는 것은 입력벡터의 차원 (≠ Layers, depth)

### 

# 더 궁금한 것

**7-5**

수연님께 단어를 자를때 길이순으로 정렬하고 패딩이 비슷한것끼리 미니배치로 묶어서 사용한다고 들었는데, keras에서 구현할 수 있나? keras에서 embedding 함수 만드려면 각 문장에 들어있는 단어의 수가 일정해야하는데 어떻게 해결하나?